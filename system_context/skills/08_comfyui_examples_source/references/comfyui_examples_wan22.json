{"category": "wan22", "entries": [{"source": "json-file", "file": "wan22/image_to_video_wan22_14B.json", "prompt": {"id": "ec7da562-7e21-4dac-a0d2-f4441e1efd3b", "revision": 0, "last_node_id": 60, "last_link_id": 126, "nodes": [{"id": 7, "type": "CLIPTextEncode", "pos": [413, 389], "size": [425.27801513671875, 180.6060791015625], "flags": {}, "order": 8, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 75}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "slot_index": 0, "links": [98]}], "title": "CLIP Text Encode (Negative Prompt)", "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70"], "color": "#322", "bgcolor": "#533"}, {"id": 54, "type": "ModelSamplingSD3", "pos": [486.4836120605469, -69.28914642333984], "size": [315, 58], "flags": {}, "order": 9, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 110}], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [125]}], "properties": {"Node name for S&R": "ModelSamplingSD3"}, "widgets_values": [8.000000000000002]}, {"id": 55, "type": "ModelSamplingSD3", "pos": [484.0019836425781, 54.46213912963867], "size": [315, 58], "flags": {}, "order": 10, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 112}], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [123]}], "properties": {"Node name for S&R": "ModelSamplingSD3"}, "widgets_values": [8]}, {"id": 58, "type": "KSamplerAdvanced", "pos": [1262.509765625, -26.73247528076172], "size": [304.748046875, 334], "flags": {}, "order": 13, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 123}, {"name": "positive", "type": "CONDITIONING", "link": 121}, {"name": "negative", "type": "CONDITIONING", "link": 122}, {"name": "latent_image", "type": "LATENT", "link": 113}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [124]}], "properties": {"Node name for S&R": "KSamplerAdvanced"}, "widgets_values": ["disable", 0, "fixed", 20, 3.5, "euler", "simple", 10, 10000, "disable"]}, {"id": 38, "type": "CLIPLoader", "pos": [30, 190], "size": [360, 106], "flags": {}, "order": 0, "mode": 0, "inputs": [], "outputs": [{"name": "CLIP", "type": "CLIP", "slot_index": 0, "links": [74, 75]}], "properties": {"Node name for S&R": "CLIPLoader"}, "widgets_values": ["umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 37, "type": "UNETLoader", "pos": [30, -70], "size": [430, 82], "flags": {}, "order": 1, "mode": 0, "inputs": [], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [110]}], "properties": {"Node name for S&R": "UNETLoader"}, "widgets_values": ["wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 56, "type": "UNETLoader", "pos": [30, 60], "size": [430, 82], "flags": {}, "order": 2, "mode": 0, "inputs": [], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [112]}], "properties": {"Node name for S&R": "UNETLoader"}, "widgets_values": ["wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 39, "type": "VAELoader", "pos": [30, 340], "size": [360, 58], "flags": {}, "order": 3, "mode": 0, "inputs": [], "outputs": [{"name": "VAE", "type": "VAE", "slot_index": 0, "links": [76, 99]}], "properties": {"Node name for S&R": "VAELoader"}, "widgets_values": ["wan_2.1_vae.safetensors"], "color": "#223", "bgcolor": "#335"}, {"id": 59, "type": "Note", "pos": [-202.05557250976562, -57.859466552734375], "size": [210, 159.49227905273438], "flags": {}, "order": 4, "mode": 0, "inputs": [], "outputs": [], "properties": {}, "widgets_values": ["This model uses a different diffusion model for the first steps (high noise) vs the last steps (low noise).\n\n"], "color": "#432", "bgcolor": "#653"}, {"id": 60, "type": "Note", "pos": [-200, 340], "size": [210, 159.49227905273438], "flags": {}, "order": 5, "mode": 0, "inputs": [], "outputs": [], "properties": {}, "widgets_values": ["This model uses the wan 2.1 VAE.\n\n\n"], "color": "#432", "bgcolor": "#653"}, {"id": 8, "type": "VAEDecode", "pos": [1590, -20], "size": [210, 46], "flags": {}, "order": 14, "mode": 0, "inputs": [{"name": "samples", "type": "LATENT", "link": 124}, {"name": "vae", "type": "VAE", "link": 76}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "slot_index": 0, "links": [56, 93]}], "properties": {"Node name for S&R": "VAEDecode"}, "widgets_values": []}, {"id": 47, "type": "SaveWEBM", "pos": [2530, -20], "size": [763.67041015625, 885.67041015625], "flags": {}, "order": 16, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 93}], "outputs": [], "properties": {"Node name for S&R": "SaveWEBM"}, "widgets_values": ["ComfyUI", "vp9", 16.000000000000004, 13.3333740234375]}, {"id": 57, "type": "KSamplerAdvanced", "pos": [893.0060424804688, -29.923471450805664], "size": [304.748046875, 334], "flags": {}, "order": 12, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 125}, {"name": "positive", "type": "CONDITIONING", "link": 118}, {"name": "negative", "type": "CONDITIONING", "link": 119}, {"name": "latent_image", "type": "LATENT", "link": 120}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [113]}], "properties": {"Node name for S&R": "KSamplerAdvanced"}, "widgets_values": ["enable", 99822389587980, "randomize", 20, 3.5, "euler", "simple", 0, 10, "enable"]}, {"id": 28, "type": "SaveAnimatedWEBP", "pos": [1820, -20], "size": [674.6224975585938, 820.6224975585938], "flags": {}, "order": 15, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 56}], "outputs": [], "properties": {}, "widgets_values": ["ComfyUI", 16, false, 80, "default"]}, {"id": 50, "type": "WanImageToVideo", "pos": [491.7362060546875, 617.798095703125], "size": [342.5999755859375, 210], "flags": {}, "order": 11, "mode": 0, "inputs": [{"name": "positive", "type": "CONDITIONING", "link": 97}, {"name": "negative", "type": "CONDITIONING", "link": 98}, {"name": "vae", "type": "VAE", "link": 99}, {"name": "clip_vision_output", "shape": 7, "type": "CLIP_VISION_OUTPUT", "link": null}, {"name": "start_image", "shape": 7, "type": "IMAGE", "link": 126}], "outputs": [{"name": "positive", "type": "CONDITIONING", "slot_index": 0, "links": [118, 121]}, {"name": "negative", "type": "CONDITIONING", "slot_index": 1, "links": [119, 122]}, {"name": "latent", "type": "LATENT", "slot_index": 2, "links": [120]}], "properties": {"Node name for S&R": "WanImageToVideo"}, "widgets_values": [768, 768, 81, 1]}, {"id": 6, "type": "CLIPTextEncode", "pos": [415, 186], "size": [422.84503173828125, 164.31304931640625], "flags": {}, "order": 7, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 74}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "slot_index": 0, "links": [97]}], "title": "CLIP Text Encode (Positive Prompt)", "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["a cute anime girl picking up an assault rifle and moving quickly"], "color": "#232", "bgcolor": "#353"}, {"id": 52, "type": "LoadImage", "pos": [-50, 550], "size": [450, 540], "flags": {}, "order": 6, "mode": 0, "inputs": [], "outputs": [{"name": "IMAGE", "type": "IMAGE", "slot_index": 0, "links": [126]}, {"name": "MASK", "type": "MASK", "slot_index": 1, "links": null}], "properties": {"Node name for S&R": "LoadImage"}, "widgets_values": ["fennec_girl_flowers.png", "image"]}], "links": [[56, 8, 0, 28, 0, "IMAGE"], [74, 38, 0, 6, 0, "CLIP"], [75, 38, 0, 7, 0, "CLIP"], [76, 39, 0, 8, 1, "VAE"], [93, 8, 0, 47, 0, "IMAGE"], [97, 6, 0, 50, 0, "CONDITIONING"], [98, 7, 0, 50, 1, "CONDITIONING"], [99, 39, 0, 50, 2, "VAE"], [110, 37, 0, 54, 0, "MODEL"], [112, 56, 0, 55, 0, "MODEL"], [113, 57, 0, 58, 3, "LATENT"], [118, 50, 0, 57, 1, "CONDITIONING"], [119, 50, 1, 57, 2, "CONDITIONING"], [120, 50, 2, 57, 3, "LATENT"], [121, 50, 0, 58, 1, "CONDITIONING"], [122, 50, 1, 58, 2, "CONDITIONING"], [123, 55, 0, 58, 0, "MODEL"], [124, 58, 0, 8, 0, "LATENT"], [125, 54, 0, 57, 0, "MODEL"], [126, 52, 0, 50, 4, "IMAGE"]], "groups": [], "config": {}, "extra": {"ds": {"scale": 1.1167815779425299, "offset": [229.4669275491141, 115.0852193902741]}, "frontendVersion": "1.23.4"}, "version": 0.4}, "workflow": null}, {"source": "json-file", "file": "wan22/image_to_video_wan22_5B.json", "prompt": {"id": "91f6bbe2-ed41-4fd6-bac7-71d5b5864ecb", "revision": 0, "last_node_id": 57, "last_link_id": 106, "nodes": [{"id": 8, "type": "VAEDecode", "pos": [1210, 190], "size": [210, 46], "flags": {}, "order": 10, "mode": 0, "inputs": [{"name": "samples", "type": "LATENT", "link": 35}, {"name": "vae", "type": "VAE", "link": 76}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "slot_index": 0, "links": [56, 93]}], "properties": {"Node name for S&R": "VAEDecode"}, "widgets_values": []}, {"id": 7, "type": "CLIPTextEncode", "pos": [413, 389], "size": [425.27801513671875, 180.6060791015625], "flags": {}, "order": 6, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 75}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "slot_index": 0, "links": [52]}], "title": "CLIP Text Encode (Negative Prompt)", "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70"], "color": "#322", "bgcolor": "#533"}, {"id": 3, "type": "KSampler", "pos": [863, 187], "size": [315, 262], "flags": {}, "order": 9, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 95}, {"name": "positive", "type": "CONDITIONING", "link": 46}, {"name": "negative", "type": "CONDITIONING", "link": 52}, {"name": "latent_image", "type": "LATENT", "link": 104}], "outputs": [{"name": "LATENT", "type": "LATENT", "slot_index": 0, "links": [35]}], "properties": {"Node name for S&R": "KSampler"}, "widgets_values": [869177064731501, "randomize", 30, 5, "uni_pc", "simple", 1]}, {"id": 28, "type": "SaveAnimatedWEBP", "pos": [1460, 190], "size": [870.8511352539062, 648.4141235351562], "flags": {}, "order": 11, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 56}], "outputs": [], "properties": {}, "widgets_values": ["ComfyUI", 24.000000000000004, false, 90, "default"]}, {"id": 39, "type": "VAELoader", "pos": [20, 340], "size": [330, 60], "flags": {}, "order": 0, "mode": 0, "inputs": [], "outputs": [{"name": "VAE", "type": "VAE", "slot_index": 0, "links": [76, 105]}], "properties": {"Node name for S&R": "VAELoader"}, "widgets_values": ["wan2.2_vae.safetensors"], "color": "#223", "bgcolor": "#335"}, {"id": 38, "type": "CLIPLoader", "pos": [20, 190], "size": [380, 106], "flags": {}, "order": 1, "mode": 0, "inputs": [], "outputs": [{"name": "CLIP", "type": "CLIP", "slot_index": 0, "links": [74, 75]}], "properties": {"Node name for S&R": "CLIPLoader"}, "widgets_values": ["umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 48, "type": "ModelSamplingSD3", "pos": [440, 60], "size": [210, 58], "flags": {}, "order": 7, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 94}], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [95]}], "properties": {"Node name for S&R": "ModelSamplingSD3"}, "widgets_values": [8.000000000000002]}, {"id": 37, "type": "UNETLoader", "pos": [20, 60], "size": [346.7470703125, 82], "flags": {}, "order": 2, "mode": 0, "inputs": [], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [94]}], "properties": {"Node name for S&R": "UNETLoader"}, "widgets_values": ["wan2.2_ti2v_5B_fp16.safetensors", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 47, "type": "SaveWEBM", "pos": [2367.213134765625, 193.6114959716797], "size": [670, 650], "flags": {}, "order": 12, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 93}], "outputs": [], "properties": {"Node name for S&R": "SaveWEBM"}, "widgets_values": ["ComfyUI", "vp9", 24, 16.111083984375]}, {"id": 57, "type": "LoadImage", "pos": [87.407958984375, 620.4816284179688], "size": [274.080078125, 314], "flags": {}, "order": 3, "mode": 0, "inputs": [], "outputs": [{"name": "IMAGE", "type": "IMAGE", "links": [106]}, {"name": "MASK", "type": "MASK", "links": null}], "properties": {"Node name for S&R": "LoadImage"}, "widgets_values": ["fennec_girl_hug.png", "image"]}, {"id": 56, "type": "Note", "pos": [710.781005859375, 608.9545288085938], "size": [320.9936218261719, 182.6057586669922], "flags": {}, "order": 4, "mode": 0, "inputs": [], "outputs": [], "properties": {}, "widgets_values": ["Optimal resolution is: 1280x704 length 121\n\nThe reason it's lower in this workflow is just because I didn't want you to wait too long to get an initial video.\n\nTo get image to video just plug in a start image. For text to video just don't give it a start image."], "color": "#432", "bgcolor": "#653"}, {"id": 55, "type": "Wan22ImageToVideoLatent", "pos": [420, 610], "size": [271.9126892089844, 150], "flags": {}, "order": 8, "mode": 0, "inputs": [{"name": "vae", "type": "VAE", "link": 105}, {"name": "start_image", "shape": 7, "type": "IMAGE", "link": 106}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [104]}], "properties": {"Node name for S&R": "Wan22ImageToVideoLatent"}, "widgets_values": [1280, 704, 41, 1]}, {"id": 6, "type": "CLIPTextEncode", "pos": [415, 186], "size": [422.84503173828125, 164.31304931640625], "flags": {}, "order": 5, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 74}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "slot_index": 0, "links": [46]}], "title": "CLIP Text Encode (Positive Prompt)", "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["a cute anime girl with fennec ears and a fluffy tail walking in a beautiful field"], "color": "#232", "bgcolor": "#353"}], "links": [[35, 3, 0, 8, 0, "LATENT"], [46, 6, 0, 3, 1, "CONDITIONING"], [52, 7, 0, 3, 2, "CONDITIONING"], [56, 8, 0, 28, 0, "IMAGE"], [74, 38, 0, 6, 0, "CLIP"], [75, 38, 0, 7, 0, "CLIP"], [76, 39, 0, 8, 1, "VAE"], [93, 8, 0, 47, 0, "IMAGE"], [94, 37, 0, 48, 0, "MODEL"], [95, 48, 0, 3, 0, "MODEL"], [104, 55, 0, 3, 3, "LATENT"], [105, 39, 0, 55, 0, "VAE"], [106, 57, 0, 55, 1, "IMAGE"]], "groups": [], "config": {}, "extra": {"ds": {"scale": 1.1167815779425287, "offset": [3.5210927484772534, -9.231468990407302]}, "frontendVersion": "1.23.4"}, "version": 0.4}, "workflow": null}, {"source": "json-file", "file": "wan22/text_to_video_wan22_14B.json", "prompt": {"id": "ec7da562-7e21-4dac-a0d2-f4441e1efd3b", "revision": 0, "last_node_id": 61, "last_link_id": 131, "nodes": [{"id": 54, "type": "ModelSamplingSD3", "pos": [486.4836120605469, -69.28914642333984], "size": [315, 58], "flags": {}, "order": 10, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 110}], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [125]}], "properties": {"Node name for S&R": "ModelSamplingSD3"}, "widgets_values": [8.000000000000002]}, {"id": 58, "type": "KSamplerAdvanced", "pos": [1262.509765625, -26.73247528076172], "size": [304.748046875, 334], "flags": {}, "order": 12, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 123}, {"name": "positive", "type": "CONDITIONING", "link": 128}, {"name": "negative", "type": "CONDITIONING", "link": 130}, {"name": "latent_image", "type": "LATENT", "link": 113}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [124]}], "properties": {"Node name for S&R": "KSamplerAdvanced"}, "widgets_values": ["disable", 0, "fixed", 20, 3.5, "euler", "simple", 10, 10000, "disable"]}, {"id": 38, "type": "CLIPLoader", "pos": [30, 190], "size": [360, 106], "flags": {}, "order": 0, "mode": 0, "inputs": [], "outputs": [{"name": "CLIP", "type": "CLIP", "slot_index": 0, "links": [74, 75]}], "properties": {"Node name for S&R": "CLIPLoader"}, "widgets_values": ["umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 39, "type": "VAELoader", "pos": [30, 340], "size": [360, 58], "flags": {}, "order": 1, "mode": 0, "inputs": [], "outputs": [{"name": "VAE", "type": "VAE", "slot_index": 0, "links": [76]}], "properties": {"Node name for S&R": "VAELoader"}, "widgets_values": ["wan_2.1_vae.safetensors"], "color": "#223", "bgcolor": "#335"}, {"id": 59, "type": "Note", "pos": [-202.05557250976562, -57.859466552734375], "size": [210, 159.49227905273438], "flags": {}, "order": 2, "mode": 0, "inputs": [], "outputs": [], "properties": {}, "widgets_values": ["This model uses a different diffusion model for the first steps (high noise) vs the last steps (low noise).\n\n"], "color": "#432", "bgcolor": "#653"}, {"id": 60, "type": "Note", "pos": [-200, 340], "size": [210, 159.49227905273438], "flags": {}, "order": 3, "mode": 0, "inputs": [], "outputs": [], "properties": {}, "widgets_values": ["This model uses the wan 2.1 VAE.\n\n\n"], "color": "#432", "bgcolor": "#653"}, {"id": 8, "type": "VAEDecode", "pos": [1590, -20], "size": [210, 46], "flags": {}, "order": 13, "mode": 0, "inputs": [{"name": "samples", "type": "LATENT", "link": 124}, {"name": "vae", "type": "VAE", "link": 76}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "slot_index": 0, "links": [56, 93]}], "properties": {"Node name for S&R": "VAEDecode"}, "widgets_values": []}, {"id": 28, "type": "SaveAnimatedWEBP", "pos": [1820, -20], "size": [674.6224975585938, 820.6224975585938], "flags": {}, "order": 14, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 56}], "outputs": [], "properties": {}, "widgets_values": ["ComfyUI", 16, false, 80, "default"]}, {"id": 47, "type": "SaveWEBM", "pos": [2530, -20], "size": [763.67041015625, 885.67041015625], "flags": {}, "order": 15, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 93}], "outputs": [], "properties": {"Node name for S&R": "SaveWEBM"}, "widgets_values": ["ComfyUI", "vp9", 16.000000000000004, 13.3333740234375]}, {"id": 7, "type": "CLIPTextEncode", "pos": [413, 389], "size": [425.27801513671875, 180.6060791015625], "flags": {}, "order": 8, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 75}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "slot_index": 0, "links": [129, 130]}], "title": "CLIP Text Encode (Negative Prompt)", "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70"], "color": "#322", "bgcolor": "#533"}, {"id": 56, "type": "UNETLoader", "pos": [30, 60], "size": [430, 82], "flags": {}, "order": 4, "mode": 0, "inputs": [], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [112]}], "properties": {"Node name for S&R": "UNETLoader"}, "widgets_values": ["wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 55, "type": "ModelSamplingSD3", "pos": [484.0019836425781, 54.46213912963867], "size": [315, 58], "flags": {}, "order": 9, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 112}], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [123]}], "properties": {"Node name for S&R": "ModelSamplingSD3"}, "widgets_values": [8]}, {"id": 37, "type": "UNETLoader", "pos": [30, -70], "size": [430, 82], "flags": {}, "order": 5, "mode": 0, "inputs": [], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [110]}], "properties": {"Node name for S&R": "UNETLoader"}, "widgets_values": ["wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 57, "type": "KSamplerAdvanced", "pos": [893.0060424804688, -29.923471450805664], "size": [304.748046875, 334], "flags": {}, "order": 11, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 125}, {"name": "positive", "type": "CONDITIONING", "link": 127}, {"name": "negative", "type": "CONDITIONING", "link": 129}, {"name": "latent_image", "type": "LATENT", "link": 131}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [113]}], "properties": {"Node name for S&R": "KSamplerAdvanced"}, "widgets_values": ["enable", 738226772790037, "randomize", 20, 3.5, "euler", "simple", 0, 10, "enable"]}, {"id": 61, "type": "EmptyHunyuanLatentVideo", "pos": [560, 620], "size": [270.0943298339844, 130], "flags": {}, "order": 6, "mode": 0, "inputs": [], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [131]}], "properties": {"Node name for S&R": "EmptyHunyuanLatentVideo"}, "widgets_values": [1280, 704, 57, 1]}, {"id": 6, "type": "CLIPTextEncode", "pos": [415, 186], "size": [422.84503173828125, 164.31304931640625], "flags": {}, "order": 7, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 74}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "slot_index": 0, "links": [127, 128]}], "title": "CLIP Text Encode (Positive Prompt)", "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["a robot is running through a futuristic cyberpunk city with neon signs and darkness with bright HDR lights"], "color": "#232", "bgcolor": "#353"}], "links": [[56, 8, 0, 28, 0, "IMAGE"], [74, 38, 0, 6, 0, "CLIP"], [75, 38, 0, 7, 0, "CLIP"], [76, 39, 0, 8, 1, "VAE"], [93, 8, 0, 47, 0, "IMAGE"], [110, 37, 0, 54, 0, "MODEL"], [112, 56, 0, 55, 0, "MODEL"], [113, 57, 0, 58, 3, "LATENT"], [123, 55, 0, 58, 0, "MODEL"], [124, 58, 0, 8, 0, "LATENT"], [125, 54, 0, 57, 0, "MODEL"], [127, 6, 0, 57, 1, "CONDITIONING"], [128, 6, 0, 58, 1, "CONDITIONING"], [129, 7, 0, 57, 2, "CONDITIONING"], [130, 7, 0, 58, 2, "CONDITIONING"], [131, 61, 0, 57, 3, "LATENT"]], "groups": [], "config": {}, "extra": {"ds": {"scale": 1.1167815779425305, "offset": [242.9977455078102, 122.98065462666187]}, "frontendVersion": "1.23.4"}, "version": 0.4}, "workflow": null}, {"source": "json-file", "file": "wan22/text_to_video_wan22_5B.json", "prompt": {"id": "91f6bbe2-ed41-4fd6-bac7-71d5b5864ecb", "revision": 0, "last_node_id": 57, "last_link_id": 106, "nodes": [{"id": 8, "type": "VAEDecode", "pos": [1210, 190], "size": [210, 46], "flags": {}, "order": 9, "mode": 0, "inputs": [{"name": "samples", "type": "LATENT", "link": 35}, {"name": "vae", "type": "VAE", "link": 76}], "outputs": [{"name": "IMAGE", "type": "IMAGE", "slot_index": 0, "links": [56, 93]}], "properties": {"Node name for S&R": "VAEDecode"}, "widgets_values": []}, {"id": 7, "type": "CLIPTextEncode", "pos": [413, 389], "size": [425.27801513671875, 180.6060791015625], "flags": {}, "order": 6, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 75}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "slot_index": 0, "links": [52]}], "title": "CLIP Text Encode (Negative Prompt)", "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70"], "color": "#322", "bgcolor": "#533"}, {"id": 3, "type": "KSampler", "pos": [863, 187], "size": [315, 262], "flags": {}, "order": 8, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 95}, {"name": "positive", "type": "CONDITIONING", "link": 46}, {"name": "negative", "type": "CONDITIONING", "link": 52}, {"name": "latent_image", "type": "LATENT", "link": 104}], "outputs": [{"name": "LATENT", "type": "LATENT", "slot_index": 0, "links": [35]}], "properties": {"Node name for S&R": "KSampler"}, "widgets_values": [285741127119524, "randomize", 30, 5, "uni_pc", "simple", 1]}, {"id": 39, "type": "VAELoader", "pos": [20, 340], "size": [330, 60], "flags": {}, "order": 0, "mode": 0, "inputs": [], "outputs": [{"name": "VAE", "type": "VAE", "slot_index": 0, "links": [76, 105]}], "properties": {"Node name for S&R": "VAELoader"}, "widgets_values": ["wan2.2_vae.safetensors"], "color": "#223", "bgcolor": "#335"}, {"id": 38, "type": "CLIPLoader", "pos": [20, 190], "size": [380, 106], "flags": {}, "order": 1, "mode": 0, "inputs": [], "outputs": [{"name": "CLIP", "type": "CLIP", "slot_index": 0, "links": [74, 75]}], "properties": {"Node name for S&R": "CLIPLoader"}, "widgets_values": ["umt5_xxl_fp8_e4m3fn_scaled.safetensors", "wan", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 48, "type": "ModelSamplingSD3", "pos": [440, 60], "size": [210, 58], "flags": {}, "order": 7, "mode": 0, "inputs": [{"name": "model", "type": "MODEL", "link": 94}], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [95]}], "properties": {"Node name for S&R": "ModelSamplingSD3"}, "widgets_values": [8.000000000000002]}, {"id": 37, "type": "UNETLoader", "pos": [20, 60], "size": [346.7470703125, 82], "flags": {}, "order": 2, "mode": 0, "inputs": [], "outputs": [{"name": "MODEL", "type": "MODEL", "slot_index": 0, "links": [94]}], "properties": {"Node name for S&R": "UNETLoader"}, "widgets_values": ["wan2.2_ti2v_5B_fp16.safetensors", "default"], "color": "#223", "bgcolor": "#335"}, {"id": 47, "type": "SaveWEBM", "pos": [2367.213134765625, 193.6114959716797], "size": [670, 650], "flags": {}, "order": 11, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 93}], "outputs": [], "properties": {"Node name for S&R": "SaveWEBM"}, "widgets_values": ["ComfyUI", "vp9", 24, 16.111083984375]}, {"id": 56, "type": "Note", "pos": [710.781005859375, 608.9545288085938], "size": [320.9936218261719, 182.6057586669922], "flags": {}, "order": 3, "mode": 0, "inputs": [], "outputs": [], "properties": {}, "widgets_values": ["Optimal resolution is: 1280x704 length 121\n\nThe reason it's lower in this workflow is just because I didn't want you to wait too long to get an initial video.\n\nTo get image to video just plug in a start image. For text to video just don't give it a start image."], "color": "#432", "bgcolor": "#653"}, {"id": 55, "type": "Wan22ImageToVideoLatent", "pos": [420, 610], "size": [271.9126892089844, 150], "flags": {}, "order": 4, "mode": 0, "inputs": [{"name": "vae", "type": "VAE", "link": 105}, {"name": "start_image", "shape": 7, "type": "IMAGE", "link": null}], "outputs": [{"name": "LATENT", "type": "LATENT", "links": [104]}], "properties": {"Node name for S&R": "Wan22ImageToVideoLatent"}, "widgets_values": [1280, 704, 41, 1]}, {"id": 6, "type": "CLIPTextEncode", "pos": [415, 186], "size": [422.84503173828125, 164.31304931640625], "flags": {}, "order": 5, "mode": 0, "inputs": [{"name": "clip", "type": "CLIP", "link": 74}], "outputs": [{"name": "CONDITIONING", "type": "CONDITIONING", "slot_index": 0, "links": [46]}], "title": "CLIP Text Encode (Positive Prompt)", "properties": {"Node name for S&R": "CLIPTextEncode"}, "widgets_values": ["drone shot of a volcano erupting with a fox walking on it"], "color": "#232", "bgcolor": "#353"}, {"id": 28, "type": "SaveAnimatedWEBP", "pos": [1460, 190], "size": [870.8511352539062, 648.4141235351562], "flags": {}, "order": 10, "mode": 0, "inputs": [{"name": "images", "type": "IMAGE", "link": 56}], "outputs": [], "properties": {}, "widgets_values": ["ComfyUI", 24.000000000000004, false, 80, "default"]}], "links": [[35, 3, 0, 8, 0, "LATENT"], [46, 6, 0, 3, 1, "CONDITIONING"], [52, 7, 0, 3, 2, "CONDITIONING"], [56, 8, 0, 28, 0, "IMAGE"], [74, 38, 0, 6, 0, "CLIP"], [75, 38, 0, 7, 0, "CLIP"], [76, 39, 0, 8, 1, "VAE"], [93, 8, 0, 47, 0, "IMAGE"], [94, 37, 0, 48, 0, "MODEL"], [95, 48, 0, 3, 0, "MODEL"], [104, 55, 0, 3, 3, "LATENT"], [105, 39, 0, 55, 0, "VAE"]], "groups": [], "config": {}, "extra": {"ds": {"scale": 1.11678157794253, "offset": [7.041966347099882, -19.733042401058505]}, "frontendVersion": "1.23.4"}, "version": 0.4}, "workflow": null}]}