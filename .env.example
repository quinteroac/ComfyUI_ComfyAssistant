# Optional: force provider selection (`openai`, `anthropic`, `claude_code`, `codex`, `gemini_cli`).
# If unset, backend auto-selects based on available credentials.
# LLM_PROVIDER=openai

# OpenAI-compatible provider configuration
OPENAI_API_KEY=sk_your_api_key_here
OPENAI_API_BASE_URL=https://api.openai.com/v1
# OPENAI_MODEL=gpt-4o-mini

# Anthropic provider configuration
# Note: this backend uses Anthropic Messages API directly and requires API key auth.
# ANTHROPIC_AUTH_TOKEN from `claude setup-token` is not accepted by this API path.
# ANTHROPIC_API_KEY=sk-ant-api03-...
# ANTHROPIC_MODEL=claude-sonnet-4-5
# ANTHROPIC_BASE_URL=https://api.anthropic.com
# ANTHROPIC_MAX_TOKENS=4096

# Claude Code CLI provider configuration
# CLAUDE_CODE_COMMAND=claude
# CLAUDE_CODE_MODEL=sonnet

# Codex CLI provider configuration
# CODEX_COMMAND=codex
# CODEX_MODEL=o3

# Gemini CLI provider configuration
# GEMINI_CLI_COMMAND=gemini
# GEMINI_CLI_MODEL=gemini-2.5-flash

# Timeout for CLI provider commands (seconds)
# CLI_PROVIDER_TIMEOUT_SECONDS=180

# Optional: delay in seconds before each LLM request (default: 1.0).
# Helps avoid 429 rate limits when the agent makes several tool calls in a row.
# LLM_REQUEST_DELAY_SECONDS=1.0

# Optional: max characters from system_context injected in each request (default: 12000).
# Lower this to reduce prompt size and latency.
# LLM_SYSTEM_CONTEXT_MAX_CHARS=12000

# Optional: max characters for formatted user context block (default: 2500).
# Includes rules, SOUL/goals, and user skills summaries.
# LLM_USER_CONTEXT_MAX_CHARS=2500

# Optional: max non-system messages sent to the LLM per request (default: 24).
# Older messages are dropped from the prompt window.
# LLM_HISTORY_MAX_MESSAGES=24

# Optional: number of "rounds" of tool results to keep in full (default: 2).
# A round = one assistant message with tool_calls + its tool replies. Older rounds get a short placeholder to avoid context growth.
# LLM_TOOL_RESULT_KEEP_LAST_ROUNDS=2

# Optional: log level for the backend ("DEBUG", "INFO", "WARNING", "ERROR").
# Default: INFO
# COMFY_ASSISTANT_LOG_LEVEL=INFO

# Optional: always emit context pipeline debug metrics (default: off).
# When "1", every /api/chat response includes X-ComfyAssistant-Context-Debug header
# and a "context-debug" SSE event with pipeline metrics (truncation, trimming, token counts).
# Also available per-request via ?debug=context query parameter.
# COMFY_ASSISTANT_DEBUG_CONTEXT=0

# Optional: enable conversation logging in user_context/logs/ (default: 0).
# When "1", interactions are saved to daily JSONL files for flow analysis.
# COMFY_ASSISTANT_ENABLE_LOGS=0

# Optional: SearXNG instance URL for web search (fallback: DuckDuckGo)
# SEARXNG_URL=http://localhost:8080

# Note: Free tiers often have rate limits (429). If you see "Rate limit exceeded",
# wait about a minute before sending again, or increase LLM_REQUEST_DELAY_SECONDS.
